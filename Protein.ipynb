{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Secondary Structure Prediction - Tensorflow\n",
    "\n",
    "Using 75/130 proteins from the  Rost-Sander database that share than 25% identity. \n",
    "\n",
    "Links:\n",
    "\n",
    "http://antheprot-pbil.ibcp.fr/Rost.html\n",
    "\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.1418&rep=rep1&type=pdf\n",
    "\n",
    "The database protein amino acid sequences for 130 protein, and their secondary structure.  Using only the amino-acid sequence, I train a Deep neural network  to predict the secondary structure of the  individual amino acid in the peptide chain.  A sliding window of 17 amino acid  as the input, and predict secondary structure of the amino acid in the middle. The window size of 17 is selected because of the correlation found between the secondary structure and eight amino acids on either sides of the central amino acid of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# The data was originally saved as .mat from matlab.\n",
    "from scipy.io import loadmat\n",
    "from scipy.sparse import kron\n",
    "from scipy.linalg import hankel\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary to convert the amino acid sequences to numeric sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa_dict = {1:'A',2:'R',3:'N',4:'D',5:'C',6:'Q',7:'E',8:'G',9:'H',10:'I',\n",
    "          11:'L',12:'K',13:'M',14:'F',15:'P',16:'S',17:'T',18:'W',19:'Y',\n",
    "          20:'V',21:'B',22:'Z',23:'X',24:'*',25:'-',26:'?'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = loadmat('/Users/Jigar/hubiC/Documents/Matlab/Ross_Saunder.mat',squeeze_me=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of proteins in the dataset = 75\n",
      "Protein id :  1CSE-ICOMPLEX(SERINEPROTEINASE-INHIBITOR)03-JU\n",
      "Protein Sequence :  KSFPEVVGKTVDQAREYFTLHYPQYNVYFLPEGSPVTLDLRYNRVRVFYNPGTNVVNHVPHVG\n",
      "Protein Structure :  CCCHHHCCCCHHHHHHHHHHHCCCCEEEEEECCCCEECCCCCCEEEEEEECCCCEECCCCEEC\n"
     ]
    }
   ],
   "source": [
    "N = data['allSeq']['Sequence'].shape[0]  \n",
    "print (\"Total no. of proteins in the dataset = {}\".format(N))\n",
    "\n",
    "#Let's look at the identity of protein_no = 6\n",
    "protein_no = 6\n",
    "protein_id = data['allSeq']['Header'][protein_no]\n",
    "seq1_n = data['allSeq']['Sequence'][protein_no]\n",
    "str1 = data['allSeq']['Structure'][protein_no]\n",
    "\n",
    "print ('Protein id : ',protein_id)\n",
    "print ('Protein Sequence : ',''.join([aa_dict.get(x) for x in seq1_n]))\n",
    "print ('Protein Structure : ', str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = 17\n",
    "win = hankel(seq1_n[:W],seq1_n[W-1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each amino acid is encoded as 1-D binary array of size 20. In this array, the element corresponding to the amino acid is set to 1 and all the other elements are set to 0. Since each input window consists of 17 amino acids, the actual input is 17 x 20 = 340 features.\n",
    "\n",
    "Similarly, the secondary structures C,E and H are convert to one hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input data : (15471, 340)\n",
      "Length of input target : (15471, 3)\n",
      "Length of Training data = (10365, 340)\n",
      "Length of Test data = (5106, 340)\n"
     ]
    }
   ],
   "source": [
    "#This part of code converts all the protein sequences to desired input format. Each amino acid in the protein \n",
    "#is represented as a 17 amino acid sliding window, with each amino acid being encoded as a 20 element 1-D array.\n",
    "#The output for each amino acid is convert to a one hot vector of size 3.\n",
    "\n",
    "X_temp = []\n",
    "y_temp = []\n",
    "for i in range(N):\n",
    "    protein_no = i\n",
    "    protein_id = data['allSeq']['Header'][protein_no]\n",
    "    seq1_n = data['allSeq']['Sequence'][protein_no]\n",
    "    str1 = data['allSeq']['Structure'][protein_no]\n",
    "    \n",
    "    win = hankel(seq1_n[:W],seq1_n[W-1:])\n",
    "    j1=kron(win,np.ones((20,1))).toarray()\n",
    "    j2 = kron(np.ones(win.shape),np.outer(np.array(np.arange(1,21)),np.ones(1))).toarray()\n",
    "    input_seq = list(j1==j2)\n",
    "    X_temp.append(input_seq)\n",
    "    \n",
    "    j3 = kron(np.ones((1,win.shape[1])),np.outer(np.array([ord(x) for x in list('CEH')]),np.ones(1))).toarray()\n",
    "    j4 = kron(np.array([ord(x) for x in list(str1[int((W-1)/2):-int((W+1)/2-1)])]),np.ones((3,1))).toarray()\n",
    "    input_struc = list(j4==j3)\n",
    "    y_temp.append(input_struc)\n",
    "\n",
    "X_data = np.transpose(np.hstack(X_temp)).astype(np.float32)\n",
    "y_data = np.transpose(np.hstack(y_temp)).astype(np.float32)\n",
    "y_labels = np.argmax(y_data,axis=1)\n",
    "print (\"Length of input data : {}\".format(X_data.shape))\n",
    "print (\"Length of input target : {}\".format(y_data.shape))\n",
    "\n",
    "\n",
    "#The data is split into training and testing datasets\n",
    "X_train_data, X_test_data,y_train_data,y_test_data = train_test_split(X_data,y_data,test_size=0.33)\n",
    "print (\"Length of Training data = {}\".format(X_train_data.shape))\n",
    "print (\"Length of Test data = {}\".format(X_test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters to using during training.\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 125\n",
    "display_step = 20\n",
    "total_batch = int(X_train_data.shape[0]/batch_size)\n",
    "\n",
    "\n",
    "#Creating placeholder to feed the neural network\n",
    "x = tf.placeholder(tf.float32,shape=[None,X_data.shape[1]])\n",
    "y = tf.placeholder(tf.float32,shape=[None,3])\n",
    "\n",
    "#Parameters of the input layer, hidden layers and output layer\n",
    "n_input = X_data.shape[1]\n",
    "n_classes = 3\n",
    "layers = [5]\n",
    "\n",
    "#Function to generate the neural network based on the Paramters\n",
    "def generate_model(n_input=n_input,n_classes=n_classes,layers=layers):\n",
    "    dimension_1 = [n_input]+layers\n",
    "    dimension_2 = layers+[n_classes]\n",
    "\n",
    "\n",
    "    names = ['h'+str(i+1) for i in range(len(layers))]+['out']\n",
    "\n",
    "    weights_var = [tf.Variable(tf.random_normal([i,j])) for i,j in zip(dimension_1,dimension_2)]\n",
    "    biases_var = [tf.Variable(tf.random_normal([i])) for i in (dimension_2)]\n",
    "\n",
    "    weights_1 = dict(zip(names,weights_var))\n",
    "    biases_1 = dict(zip(names,biases_var))\n",
    "\n",
    "    layer = x\n",
    "\n",
    "    for key in sorted(weights_1):\n",
    "        print (weights_1[key])\n",
    "        layer = tf.add(tf.matmul(layer,weights_1[key]),biases_1[key])\n",
    "        #if key != 'out':\n",
    "            #layer = tf.nn.relu(layer) \n",
    "    \n",
    "    return layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate batches for input during the training\n",
    "def get_batch(x,y):\n",
    "    N =x.shape[0]\n",
    "    idx = np.random.choice(N,size=batch_size)\n",
    "    x_batch = x[idx]\n",
    "    y_batch = y[idx]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model and defining the loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable_110/read:0\", shape=(340, 20), dtype=float32)\n",
      "Tensor(\"Variable_111/read:0\", shape=(20, 10), dtype=float32)\n",
      "Tensor(\"Variable_112/read:0\", shape=(10, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = generate_model(layers=[20,10])# deep_net(x, weights, biases)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(model,y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural network and predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, cost : 12.434651642310913\n",
      "Epoch: 20, cost : 0.885101211507146\n",
      "Epoch: 40, cost : 0.8614070524529716\n",
      "Epoch: 60, cost : 0.838609508624891\n",
      "Epoch: 80, cost : 0.8277248427635284\n",
      "Optimization Finished!\n",
      "Accuracy: 0.599882\n",
      "\n",
      "Confusion matrix : \n",
      "[[1762  273  348]\n",
      " [ 377  491  190]\n",
      " [ 557  298  810]]\n",
      "\n",
      "Precision score : 0.5967623021778418\n",
      "\n",
      "Recall score : 0.599882491186839\n",
      "\n",
      "f1 score : 0.5951250973021804\n",
      "\n",
      " Classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.74      0.69      2383\n",
      "          1       0.46      0.46      0.46      1058\n",
      "          2       0.60      0.49      0.54      1665\n",
      "\n",
      "avg / total       0.60      0.60      0.60      5106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        for i in range (total_batch):\n",
    "            batch_x, batch_y = get_batch(X_train_data,y_train_data)\n",
    "            _,l = sess.run([optimizer,loss],feed_dict={x:batch_x,y:batch_y})\n",
    "            avg_loss += l/total_batch\n",
    "        \n",
    "        if epoch%display_step == 0:\n",
    "            print (\"Epoch: {0}, cost : {1}\".format(epoch,avg_loss))\n",
    "    print (\"Optimization Finished!\")\n",
    "    #x_test, y_test = get_batch(X_data,y_data)\n",
    "    \n",
    "    x_test, y_test = X_test_data, y_test_data\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(model,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({x: x_test, y: y_test}))  \n",
    "    \n",
    "    y_pred = sess.run(tf.arg_max(model,1),feed_dict={x: x_test, y: y_test})\n",
    "    y_true = np.argmax(y_test,1)\n",
    "    \n",
    "    cm = confusion_matrix(y_true=y_true,y_pred=y_pred)\n",
    "    print (\"\\nConfusion matrix : \\n{}\".format(cm))\n",
    "    \n",
    "    print (\"\\nPrecision score : {}\".format(precision_score(y_true,y_pred,average='weighted')))\n",
    "    print (\"\\nRecall score : {}\".format(recall_score(y_true,y_pred,average='weighted')))\n",
    "    print (\"\\nf1 score : {}\".format(f1_score(y_true,y_pred,average='weighted')))\n",
    "    \n",
    "    c_report = classification_report(y_true,y_pred)\n",
    "    print (\"\\n Classification report : \\n{}\".format(c_report))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The accuracy is ~60%. Currently, there are several advanced algorithms that have achieved higher prediction accuracy taking not only the amino acid sequence, but also comparing to other similar proteins in the protein database as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
